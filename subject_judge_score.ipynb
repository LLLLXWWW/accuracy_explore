{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f773cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# 正式分析脚本 #################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Optional, Union\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(hosts=\"10.0.1.107\", http_auth=(\"elastic\", \"8ec1f622cdc1f07c7b21beeeaf8d21f3\"), port=9200, timeout=30, max_retries=3, retry_on_timeout=True)\n",
    "\n",
    "file_paths = [\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1011_1828855(1).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1011_1828855(2).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1011_1828855(3).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1011_1836221(1).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1011_1836221(2).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1011_1836221(3).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1012_1822311(1).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1012_1836223(1).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1012_1836223(2).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1013_1828837(1).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1013_1828837(2).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1013_1835723(1).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1013_1835723(2).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1013_1835723(3).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1013_1835723(2).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1015_1835285(1).xlsx\",\n",
    "        \"C:\\\\Users\\\\xxxlll\\\\Desktop\\\\东奥\\\\主观题判分数据\\\\1004_1015_1822327(1).xlsx\",\n",
    "        ]\n",
    "results = []\n",
    "subject_name = pd.DataFrame({\n",
    "    \"subject_id\": [1010, 1011, 1012, 1013, 1014, 1015],\n",
    "    \"subject_name\": [\"审计\",\"税法\", \"经济法\", \"财管\", \"会计\", \"战略\"],\n",
    "})\n",
    "\n",
    "for idx, path in enumerate(file_paths):\n",
    "    df = pd.read_excel(path)\n",
    "    # 模型得分列识别\n",
    "    model_score_col = None\n",
    "    for name in [\"AI打分\", \"系统判分\", \"程序-用户得分\"]:\n",
    "        if name in df.columns:\n",
    "            model_score_col = name\n",
    "            break\n",
    "\n",
    "    # 最小得分点提取函数\n",
    "    def extract_min_point(row):\n",
    "        for json_col in [\"AI输出\", \"结果\", \"程序判分数据\"]:\n",
    "            if json_col in df.columns and isinstance(row.get(json_col), str):\n",
    "                try:\n",
    "                    corrected_json_str = row[json_col].replace(\"null\", 'None')\n",
    "                    corrected_json_str1 =  re.sub(r'\\s+', '', corrected_json_str)\n",
    "                    json_col_res = eval(corrected_json_str1)\n",
    "                    data = json_col_res\n",
    "                    try:\n",
    "                        question_type = data['question_name']\n",
    "                        scores = [item['scores']['score'] for item in data['correct_answers_scores'] if 'scores' in item and 'score' in item.get('scores', {})]\n",
    "                        min_score = min(scores) if scores else None\n",
    "                        lose_points_messages = []\n",
    "                        # 提取 correct_answers_scores 中非空的 lose_points\n",
    "                        for item in data.get('student_scores', []):\n",
    "                            if 'scores' in item and item['scores'].get('lose_points'):\n",
    "                                for lp in item['scores']['lose_points']:\n",
    "                                    if 'message' in lp and 'score' in lp:\n",
    "                                        lose_points_messages.append(f\"{lp['message']}_{lp['score']}\")\n",
    "                        lose_point_res = '|'.join(map(str, lose_points_messages)) if lose_points_messages else ''\n",
    "                        return min_score, lose_point_res, question_type\n",
    "                    except KeyError as e:\n",
    "                        print(corrected_json_str)\n",
    "                        print(f\"Key error: {e}\")\n",
    "                        return None, '', ''\n",
    "                finally:\n",
    "                    pass\n",
    "        pattern = re.compile(r\"程序-正确.*分值\\d*\")\n",
    "        candidates = [row[col] for col in row.index if pattern.match(col) and pd.notna(row[col])]\n",
    "        return min(candidates) if candidates else None, '', ''\n",
    "\n",
    "    df[['最小得分点', '失分原因', \"判分类型\"]] = df.apply(lambda row: extract_min_point(row), axis=1, result_type='expand')\n",
    "\n",
    "    # 提取指定列\n",
    "    parsed = df[[\"科目ID\", \"用户ID\", \"试题ID\", \"试题分数\", \"判分类型\", \"boss-用户得分\", model_score_col, \"最小得分点\", \"失分原因\"]].copy()\n",
    "    parsed.columns = [\"subject_id\", \"user_id\", \"question_id\", \"question_score\", \"question_type\", \"teacher_score\", \"model_score\", \"min_point\", \"lose_points\"]\n",
    "    paper_id_info =  path.rsplit('.', 1)[0].rsplit('_', 1)[-1] # re.search(r'_(\\d+?)\\(', path)\n",
    "    if paper_id_info:\n",
    "        # 提取并打印找到的数字\n",
    "        paper_id = paper_id_info  # paper_id_info.group(1)\n",
    "        print(f\"提取的试卷id是: {paper_id}\")\n",
    "    else:\n",
    "        paper_id  = 0\n",
    "        print(\"没有找到匹配的数字\")\n",
    "    parsed[\"paper_id\"] = paper_id\n",
    "    results.append(parsed)\n",
    "\n",
    "def modify_paper_info(row):\n",
    "    original_str = row['paper_id']\n",
    "    subject = row['subject_name']\n",
    "    \n",
    "    # 检查是否以 (1), (2), 或 (3) 结尾\n",
    "    if original_str.endswith('(1)'):\n",
    "        return original_str + '_初版'\n",
    "    elif (subject == '税法' or subject == '财管'):\n",
    "        if original_str.endswith('(2)'):\n",
    "            return original_str + '_正确答案校队'\n",
    "        elif original_str.endswith('(3)'):\n",
    "            return original_str + '_修改模型占比'\n",
    "    elif subject == '经济法':\n",
    "        if original_str.endswith('(2)'):\n",
    "            return original_str + '_纠正正确答案得分'\n",
    "    \n",
    "    # 如果不满足上述任何条件，则保持原样\n",
    "    return original_str\n",
    "\n",
    "final_df = pd.concat(results, ignore_index=True)\n",
    "final_df = final_df.merge(subject_name, on=\"subject_id\")\n",
    "final_df.drop(columns=[\"subject_id\"], inplace=True)\n",
    "final_df['paper_id'] = final_df.apply(modify_paper_info, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01de24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# 所有分析函数 ##################\n",
    "def process_data(df, rule2_quantile=0.95):\n",
    "    \"\"\"定义一个通用函数来填充缺失值，并计算三种误差规则\"\"\"\n",
    "    def fill_na_min_point(data):  # 填充缺失的min_point值\n",
    "        df = data.copy()\n",
    "        na_rows = df['min_point'].isna()\n",
    "        for idx in df[na_rows].index:\n",
    "            qid = df.loc[idx, 'question_id']\n",
    "            replacement_value = df.loc[df['question_id'] == qid, 'min_point'].dropna().values\n",
    "            if len(replacement_value) > 0:\n",
    "                df.at[idx, 'min_point'] = replacement_value[0]\n",
    "        return df\n",
    "    \n",
    "    df1 = fill_na_min_point(df)\n",
    "    \n",
    "    # 计算score_error\n",
    "    df1['score_error'] = df1.apply(lambda row: row['model_score'] - row['teacher_score'], axis=1)\n",
    "    \n",
    "    # 规则1: min(question_score * 0.1, min_point)\n",
    "    df1['accept_error_rule1'] = df1.apply(lambda row: min(row['question_score'] * 0.1, row['min_point']) if not pd.isna(row['min_point']) else (row['question_score'] * 0.1), axis=1)\n",
    "\n",
    "    # 规则2: 使用分位数（例如：95%）\n",
    "    rule2_values = df1.groupby(['subject_name', 'paper_id'])['score_error'].quantile(rule2_quantile)\n",
    "    df1 = df1.merge(rule2_values.rename('accept_error_rule2'), on=['subject_name', 'paper_id'])\n",
    "    \n",
    "    # 规则3: 使用均值加减标准差\n",
    "    group_stats = df1.groupby(['subject_name', 'paper_id'])['score_error'].agg(['mean', 'std'])\n",
    "    df1 = df1.merge(group_stats, on=['subject_name', 'paper_id'])\n",
    "    df1['accept_error_rule3'] = df1.apply(lambda x: (\"_\").join([str(x['mean'] - x['std']), str(x['mean'] + x['std'])]), axis=1)\n",
    "    df2 = df1.drop(columns=['mean', 'std'], axis=1)\n",
    "    return df2\n",
    "\n",
    "def find_group_id(question_list: List[str]) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"查找题目的大题ID\"\"\"\n",
    "    body = {\n",
    "        \"query\": {\n",
    "            \"terms\": {\n",
    "                \"question_id\": question_list\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"group_id\", \"question_id\"],\n",
    "        \"size\": 10000\n",
    "    }\n",
    "    response = es.search(index='biz_question_v2', body=body)\n",
    "    if response['hits']['total']['value'] == 0:\n",
    "        return None\n",
    "    return pd.DataFrame([hit['_source'] for hit in response['hits']['hits']])\n",
    "\n",
    "def get_group_ques_df(df):\n",
    "    \"\"\"从小题层面聚合提取大题层面数据\"\"\"\n",
    "    question_list = df['question_id'].unique().tolist()\n",
    "    group_df = find_group_id(question_list)\n",
    "    \n",
    "    if group_df is not None:\n",
    "        df1 = df.merge(group_df, on='question_id', how='left')\n",
    "        df1['group_id'] = df1.apply(\n",
    "            lambda x: int(x['group_id']) if pd.notna(x['group_id']) else int(x['question_id']),\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        df1 = df.copy()\n",
    "        df1['group_id'] = df1['question_id'].astype(int)\n",
    "    \n",
    "    agg_dict = {\n",
    "        'question_score': 'sum',\n",
    "        'question_type': 'unique',\n",
    "        'teacher_score': 'sum',\n",
    "        'model_score': 'sum',\n",
    "        'min_point': 'sum'\n",
    "    }\n",
    "    \n",
    "    df2 = df1.groupby(['subject_name', 'paper_id', 'user_id', 'group_id']).agg(agg_dict).reset_index()\n",
    "    df2.rename(columns={\"group_id\": \"question_id\"}, inplace=True)\n",
    "    df2.columns = ['_'.join(col).strip() if type(col) is tuple else col for col in df2.columns.values]\n",
    "    df2.rename(columns={\n",
    "        'subject_name_': 'subject_name',\n",
    "        'paper_id_': 'paper_id',\n",
    "        'user_id_': 'user_id',\n",
    "        'question_id_': 'question_id',\n",
    "        'question_score_sum': 'question_score',\n",
    "        'question_type_unique': 'question_score',\n",
    "        'teacher_score_sum': 'teacher_score',\n",
    "        'model_score_sum': 'model_score',\n",
    "        'min_point_sum': 'min_point'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    return df2\n",
    "\n",
    "def process_dataframe(df, exclude_columns):\n",
    "    \"\"\"将结果转换成百分比\"\"\"\n",
    "    for col in df.columns:\n",
    "        if col not in exclude_columns:\n",
    "            # 执行乘以100，四舍五入到两位小数\n",
    "            df[col] = (df[col] * 100).round(2)\n",
    "            # 将结果转换为字符串并加上百分号\n",
    "            df[col] = df[col].astype(str) + '%'\n",
    "    result = df.reset_index(drop=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def stats_describe(df):\n",
    "    \"\"\"计算误差描述统计量\"\"\"\n",
    "    error_stats_ques = df.groupby(['subject_name', 'paper_id'])['score_error'].agg([\n",
    "        ('平均误差', 'mean'),\n",
    "        ('误差中位数', 'median'),\n",
    "        ('误差标准差', 'std'),\n",
    "        ('最小误差', 'min'),\n",
    "        ('最大误差', 'max'),\n",
    "        ('误差绝对值均值', lambda x: x.abs().mean()),\n",
    "        ('25%分位数', lambda x: x.quantile(0.25)),\n",
    "        ('75%分位数', lambda x: x.quantile(0.75)),\n",
    "        ('95%分位数', lambda x: x.quantile(0.95))\n",
    "    ]).reset_index(drop=False)\n",
    "    return error_stats_ques\n",
    "\n",
    "def calculate_percentage_within_margin(prob: float, group: pd.DataFrame) -> float:\n",
    "    \"\"\"计算误差在给定百分比范围内的比例\"\"\"\n",
    "    lower_bound = -prob * group['question_score']\n",
    "    upper_bound = prob * group['question_score']\n",
    "    within_margin = ((group['score_error'] >= lower_bound) & (group['score_error'] <= upper_bound))\n",
    "    return within_margin.mean()\n",
    "\n",
    "def classify_error(error: float) -> str:\n",
    "    \"\"\"分类误差类型\"\"\"\n",
    "    if error == 0:\n",
    "        return '完全匹配'\n",
    "    return '模型偏高' if error > 0 else '模型偏低'\n",
    "    \n",
    "def is_precise_match(error: float, acceptable_error: Union[float, str], rule_type: int) -> bool:\n",
    "    \"\"\"检查是否为精准匹配\"\"\"\n",
    "    if rule_type == 3:\n",
    "        lower_bound, upper_bound = map(float, acceptable_error.split('_'))\n",
    "        return lower_bound <= error <= upper_bound\n",
    "    return -acceptable_error <= error <= acceptable_error\n",
    "\n",
    "def calculate_percentage_error(row: pd.Series, percentage: float) -> bool:\n",
    "    \"\"\"计算误差是否在给定百分比范围内\"\"\"\n",
    "    question_score = row['question_score']\n",
    "    score_error = row['score_error']\n",
    "    margin = question_score * percentage / 100.0\n",
    "    return -margin <= score_error <= margin\n",
    "\n",
    "def _calculate_common_metrics(grouped_data: pd.core.groupby.generic.DataFrameGroupBy, group_key: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"计算通用指标（内部函数）\"\"\"\n",
    "    # 初始化结果DataFrame\n",
    "    stats = grouped_data.size().reset_index(name='count')\n",
    "    \n",
    "    # 计算各类误差比例\n",
    "    error_classes = grouped_data['error_class'].value_counts(normalize=True).unstack().fillna(0)\n",
    "    stats = stats.merge(error_classes, left_on=group_key, right_index=True)\n",
    "    \n",
    "    # 计算精准匹配的比例（针对三种规则）\n",
    "    for rule_num in [1, 2, 3]:\n",
    "        stats[f'精准匹配{rule_num}'] = grouped_data[f'is_precise_match_{rule_num}'].mean().values\n",
    "    \n",
    "    # 统计不同误差范围内的比例\n",
    "    error_ranges = {\n",
    "        '误差0.5内': (-0.5, 0.5),\n",
    "        '误差0.8内': (-0.8, 0.8),\n",
    "        '误差1内': (-1, 1)\n",
    "    }\n",
    "    \n",
    "    for name, (lower, upper) in error_ranges.items():\n",
    "        stats[name] = grouped_data['score_error'].apply(\n",
    "            lambda x: ((x >= lower) & (x <= upper)).mean()\n",
    "        ).values\n",
    "    \n",
    "    # 添加误差分值5%、10%、15%内的比例\n",
    "    for percent in [5, 10, 15]:\n",
    "        column_name = f'误差分值{percent}%内'\n",
    "        stats[column_name] = grouped_data.apply(\n",
    "            lambda g: g.apply(calculate_percentage_error, percentage=percent, axis=1).mean()\n",
    "        ).values\n",
    "    \n",
    "    stats['老师0分_模型给分'] = grouped_data.apply(\n",
    "        lambda g: ((g['teacher_score'] == 0) & (g['model_score'] > 0)).mean()\n",
    "    ).values\n",
    "    \n",
    "    # 新增指标：老师给分但模型打0分的数据占比\n",
    "    stats['老师给分_模型0分'] = grouped_data.apply(\n",
    "        lambda g: ((g['teacher_score'] > 0) & (g['model_score'] == 0)).mean()\n",
    "    ).values\n",
    "    \n",
    "    return stats.drop(columns=['count'])\n",
    "\n",
    "def analyze_paper_scores(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"分析试卷得分\"\"\"\n",
    "    # 转换数据类型\n",
    "    df['question_score'] = pd.to_numeric(df['question_score'], errors='coerce')\n",
    "    df['score_error'] = pd.to_numeric(df['score_error'], errors='coerce')\n",
    "    \n",
    "    # 移除无效数据\n",
    "    df = df.dropna(subset=['question_score', 'score_error'])\n",
    "    \n",
    "    # 应用分类\n",
    "    df['error_class'] = df['score_error'].apply(classify_error)\n",
    "    \n",
    "    # 针对三种规则分别计算精准匹配\n",
    "    for rule_num in [1, 2, 3]:\n",
    "        df[f'is_precise_match_{rule_num}'] = df.apply(\n",
    "            lambda x: is_precise_match(x['score_error'], x[f'accept_error_rule{rule_num}'], rule_num),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # 按试卷分组并计算指标\n",
    "    return _calculate_common_metrics(df.groupby(['subject_name','paper_id']), ['subject_name', 'paper_id'])\n",
    "\n",
    "def analyze_by_student_level(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"按学生试卷分数段水平分析\"\"\"\n",
    "    # 计算每份试卷的满分\n",
    "    paper_full_scores = df.drop_duplicates(['subject_name', 'paper_id', 'question_id'])\\\n",
    "                         .groupby(['subject_name', 'paper_id'])['question_score'].sum()\\\n",
    "                         .reset_index(name='full_score')\n",
    "    \n",
    "    # 合并回原始数据\n",
    "    df = df.merge(paper_full_scores, on=['subject_name', 'paper_id'])\n",
    "    \n",
    "    # 计算每个学生在每份试卷上的总分\n",
    "    student_scores = df.groupby(['subject_name', 'paper_id', 'user_id']).agg({\n",
    "        'teacher_score': 'sum',\n",
    "        'model_score': 'sum',\n",
    "        'full_score': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # 划分等级\n",
    "    def classify_student(row: pd.Series) -> str:\n",
    "        ratio = row['teacher_score'] / row['full_score']\n",
    "        if ratio >= 0.7:\n",
    "            return '70%以上'\n",
    "        return '30%~70%' if ratio >= 0.3 else '30%以下'\n",
    "    \n",
    "    student_scores['level'] = student_scores.apply(classify_student, axis=1)\n",
    "    \n",
    "    # 合并等级信息回原始数据\n",
    "    df = df.merge(student_scores[['subject_name', 'paper_id', 'user_id', 'level']], \n",
    "                 on=['subject_name', 'paper_id', 'user_id'])\n",
    "    \n",
    "    # 应用分类\n",
    "    df['error_class'] = df['score_error'].apply(classify_error)\n",
    "    \n",
    "    # 针对三种规则分别计算精准匹配\n",
    "    for rule_num in [1, 2, 3]:\n",
    "        df[f'is_precise_match_{rule_num}'] = df.apply(\n",
    "            lambda x: is_precise_match(x['score_error'], x[f'accept_error_rule{rule_num}'], rule_num),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # 按试卷和等级分组并计算指标\n",
    "    level_stats = _calculate_common_metrics(df.groupby(['subject_name', 'paper_id', 'level']), \n",
    "                                           ['subject_name', 'paper_id', 'level'])\n",
    "    \n",
    "    # 将 level 转换为有序分类并排序\n",
    "    level_order = ['70%以上', '30%~70%', '30%以下']\n",
    "    level_stats['level'] = pd.Categorical(level_stats['level'], categories=level_order, ordered=True)\n",
    "    return level_stats.sort_values(['subject_name', 'paper_id', 'level']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78970754",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## 主流程\n",
    "def analyze_and_describe(df, group_keys):\n",
    "    \"\"\"\n",
    "    分析数据并描述统计数据。\n",
    "    \"\"\"\n",
    "    acc_stats = analyze_paper_scores(df)\n",
    "    stats = stats_describe(df)\n",
    "    processed_acc_stats = process_dataframe(acc_stats, group_keys)\n",
    "    return processed_acc_stats, stats\n",
    "\n",
    "# step1：数据处理--误差计算\n",
    "final_df1 = process_data(final_df)\n",
    "final_group_df1 = process_data(get_group_ques_df(final_df))\n",
    "\n",
    "# step2：准确率计算\n",
    "ques_acc_stats, error_stats_ques = analyze_and_describe(final_df1, ['subject_name', 'paper_id'])\n",
    "group_acc_stats, error_stats_group = analyze_and_describe(final_group_df1, ['subject_name', 'paper_id'])\n",
    "# level_stats, _ = analyze_and_describe(final_df1, ['subject_name', 'paper_id', \"level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5beb88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据进行一定的清洗过滤后计算准确率\n",
    "# step1：先筛选每个科目最新的一份试卷数据\n",
    "sub_paper_newest = ques_acc_stats[['subject_name',\"paper_id\"]].drop_duplicates()\n",
    "sub_paper_newest1 = sub_paper_newest.copy()\n",
    "sub_paper_newest1['paper_id_info'] = sub_paper_newest1['paper_id'].str.split('(', n=1).str[0]\n",
    "sub_paper_newest2 = sub_paper_newest1.groupby(['subject_name', 'paper_id_info']).apply(lambda x: x.sort_values('paper_id',ascending=False).head(1))\n",
    "\n",
    "# step2: 根据一定的规则对数据进行过滤\n",
    "def filter_and_process_data(df_raw, paper_range, condition):\n",
    "    \"\"\"\n",
    "    根据给定条件过滤数据并处理。\n",
    "    \"\"\"\n",
    "    df_case = df_raw.copy()\n",
    "    df_case1 = df_case[df_case['paper_id'].isin(paper_range)]\n",
    "    if condition:\n",
    "        # 筛选规则1：财管中表格类型、模型判0分的数据踢掉\n",
    "        df_deal = df_case1.copy()\n",
    "        df_filtered = df_deal[~((df_deal['question_type'] == '表格') & (df_deal['model_score'] == 0))] \n",
    "        \n",
    "        # 筛选规则2：战略中lose_points为“正确答案已经全部匹配”的数据误差改为0\n",
    "        df_filtered1 = df_filtered.copy()\n",
    "        df_filtered1['is_correct_fitted'] = df_filtered1.apply(lambda x: 1 if ('正确答案已经全部匹配' in x['lose_points'] and x['subject_name'] == '战略') else 0, axis=1)\n",
    "        \n",
    "        # 筛选规则3：错误类型只包含单位错误的数据的误差改为0\n",
    "        def check_unit_error_only(lose_points):\n",
    "            # 使用正则表达式匹配仅包含“单位错误”的情况\n",
    "            if re.fullmatch(r'(?:单位错误_.+?)(?:\\|单位错误_.+?)*', lose_points):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        df_filtered2 = df_filtered1.copy()\n",
    "        df_filtered2[\"is_unit_error\"] = df_filtered2['lose_points'].apply(check_unit_error_only)\n",
    "\n",
    "        # 筛选规则4：学生答案中score、student_score都为0分但echo_txt不为空的数据代表步骤分\n",
    "        \n",
    "        # 修改符合筛选规则数据的误差值        \n",
    "        df_filtered2['model_score'] = df_filtered2.apply(lambda x: x['teacher_score'] if x['is_correct_fitted'] == 1 or x['is_unit_error'] == 1 else x['model_score'], axis=1)\n",
    "        df_filtered_res = df_filtered2.drop(columns=['is_correct_fitted', 'is_unit_error'], axis=1)\n",
    "    else:\n",
    "        df_filtered_res = df_case1.copy()\n",
    "    return df_filtered_res\n",
    "\n",
    "# step3: 计算准确率\n",
    "paper_range = sub_paper_newest2['paper_id'].unique().tolist()\n",
    "final_df1_adj = process_data(filter_and_process_data(final_df, paper_range, condition=True))\n",
    "final_group_df1_adj = process_data(get_group_ques_df(final_df1_adj))\n",
    "ques_acc_stats_adj, error_stats_ques_adj = analyze_and_describe(final_df1_adj, ['subject_name', 'paper_id'])\n",
    "group_acc_stats_adj, error_stats_group_adj = analyze_and_describe(final_group_df1_adj, ['subject_name', 'paper_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6bd1e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并数据：left join 保留原始顺序\n",
    "def format_colored(row, col):\n",
    "    raw_val = row[col]\n",
    "    filtered_val = row[f\"{col}_filtered\"]\n",
    "    if pd.isnull(filtered_val):\n",
    "        return raw_val  # 没有过滤后数据\n",
    "\n",
    "    # 转换为数值（去掉百分号）\n",
    "    raw_num = float(str(raw_val).replace('%', ''))\n",
    "    filtered_num = float(str(filtered_val).replace('%', ''))\n",
    "\n",
    "    # 比较并添加颜色\n",
    "    if abs(raw_num - filtered_num) < 1e-6:\n",
    "            return raw_val  # 无变化\n",
    "\n",
    "    color = \"green\" if filtered_num > raw_num else \"red\"\n",
    "    return f'{raw_val} <span style=\"color:{color}\">（{filtered_val}）</span>'\n",
    "\n",
    "def merge_more_accurate(df_raw, df_filter):\n",
    "    key_cols = ['subject_name', 'paper_id']\n",
    "    df_merged = df_raw.merge(df_filter, on=key_cols, suffixes=('', '_filtered'), how='left')\n",
    "\n",
    "    # 拼接百分比字段\n",
    "    accuracy_cols = [col for col in df_raw.columns if col not in key_cols]\n",
    "\n",
    "    # 应用格式化\n",
    "    for col in accuracy_cols:\n",
    "        df_merged[col] = df_merged.apply(lambda row: format_colored(row, col), axis=1)\n",
    "\n",
    "    # 保留最终列\n",
    "    df_final = df_merged[key_cols + accuracy_cols]\n",
    "    return df_final\n",
    "\n",
    "ques_acc_stats_raw = ques_acc_stats[ques_acc_stats['paper_id'].isin(paper_range)].copy()\n",
    "group_acc_stats_raw = group_acc_stats[group_acc_stats['paper_id'].isin(paper_range)].copy()\n",
    " \n",
    "ques_acc_compare = merge_more_accurate(ques_acc_stats_raw, ques_acc_stats_adj)\n",
    "group_acc_compare = merge_more_accurate(group_acc_stats_raw, group_acc_stats_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "54eeecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义表格数据html格式处理函数\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "\n",
    "def styled_group_table(df: pd.DataFrame, subject_col='科目名称') -> str:\n",
    "    # 确保不会修改原始 df\n",
    "    df1 = df.copy()\n",
    "\n",
    "    # 获取每个科目（subject_name）组的最后一行的索引\n",
    "    subject_series = df1[subject_col]\n",
    "    is_last_of_group = subject_series != subject_series.shift(-1)  # 最后一行标记\n",
    "    is_first_of_group = subject_series != subject_series.shift(1)  # 第一行标记\n",
    "\n",
    "    # 清除非首行的科目名称\n",
    "    df1.loc[~is_first_of_group, subject_col] = ''\n",
    "\n",
    "    # 创建 HTML 表格\n",
    "    html = \"\"\"\n",
    "    <style>\n",
    "        th, td { text-align: center; padding: 6px; }\n",
    "        th { border-bottom: 2px solid black; }\n",
    "        .group-end td { border-bottom: 2px solid #333; } /* 仅在最后一行添加下框线 */\n",
    "        table { border-collapse: collapse; width: 100%; margin-top: 10px; }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    \n",
    "    # 表格开头\n",
    "    html += '<table><thead><tr>'\n",
    "    html += ''.join(f'<th>{col}</th>' for col in df1.columns)\n",
    "    html += '</tr></thead><tbody>'\n",
    "\n",
    "    # 逐行生成表格\n",
    "    for i in range(len(df1)):\n",
    "        row = df1.iloc[i]\n",
    "        # 如果是最后一行，则添加下框线\n",
    "        row_class = 'group-end' if is_last_of_group.iloc[i] else ''\n",
    "        html += f'<tr class=\"{row_class}\">' + ''.join(f'<td>{row[col]}</td>' for col in df1.columns) + '</tr>'\n",
    "\n",
    "    # 结束标签\n",
    "    html += '</tbody></table>'\n",
    "    \n",
    "    return html\n",
    "\n",
    "def format_floats_smart(df: pd.DataFrame, skip_cols=2) -> pd.DataFrame:\n",
    "    df1 = df.copy()\n",
    "    num_cols = df1.select_dtypes(include='number').columns.tolist()\n",
    "    format_cols = df1.columns[skip_cols:]  # 从第 skip_cols 列之后开始格式化\n",
    "\n",
    "    for col in format_cols:\n",
    "        if col in num_cols:\n",
    "            df1[col] = df1[col].apply(lambda x: f\"{x:.3f}\".rstrip('0').rstrip('.') if pd.notnull(x) else '')\n",
    "    return df1\n",
    "\n",
    "def process_dataframe(df, skip_cols=2):\n",
    "    \"\"\"统一处理数据框：格式化浮点数并重命名列\"\"\"\n",
    "    formatted_df = format_floats_smart(df, skip_cols=skip_cols)\n",
    "    formatted_df.rename(columns={'subject_name': '科目名称', 'paper_id': '试卷ID'}, inplace=True)\n",
    "    return formatted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计试卷基本情况（题目数量，总记录条数， 题目最大分值、题目最小分值、均分）\n",
    "def paper_base_info(df, type, paper_range):\n",
    "    df_case = df[df['paper_id'].isin(paper_range)].copy()\n",
    "    # Step 1: 计算基础指标\n",
    "    basic_stats = df_case.groupby(['subject_name', 'paper_id']).agg(\n",
    "        total_records=('question_id', 'count'),\n",
    "        unique_users=('user_id', 'nunique'),\n",
    "        unique_questions=('question_id', 'nunique')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Step 2: 去重题目数据并计算分数统计\n",
    "    unique_questions_df = df_case[['subject_name', 'paper_id', 'question_id', 'question_score']].drop_duplicates()\n",
    "    score_stats = unique_questions_df.groupby(['subject_name', 'paper_id']).agg(\n",
    "        avg_question_score=('question_score', 'mean'),\n",
    "        max_question_score=('question_score', 'max'),\n",
    "        min_question_score=('question_score', 'min')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Step 3: 合并两个结果集\n",
    "    final_stats = pd.merge(basic_stats, score_stats, on=['subject_name', 'paper_id'])\n",
    "    final_stats['type'] = 1\n",
    "    return final_stats\n",
    "ques_base_count = paper_base_info(final_df1, 1, paper_range)\n",
    "group_base_count = paper_base_info(final_group_df1, 2, paper_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e174fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 表格html结果 #########\n",
    "dataframes = {\n",
    "    'html_ques_error': (error_stats_ques, 2),\n",
    "    'html_group_error': (error_stats_group, 2),\n",
    "    'html_ques_acc': (ques_acc_stats, 2),\n",
    "    'html_group_acc': (group_acc_stats, 2),\n",
    "    'compare_ques_acc': (ques_acc_compare, 2),\n",
    "    'compare_group_acc': (group_acc_compare, 2),\n",
    "}\n",
    "\n",
    "# 使用字典推导式处理所有数据框\n",
    "processed = {name: styled_group_table(process_dataframe(df, skip_cols)) \n",
    "             for name, (df, skip_cols) in dataframes.items()}\n",
    "\n",
    "# 构建最终HTML列表\n",
    "table_html_list = [\n",
    "    \"<h2>试卷整体误差统计描述</h2>\",\n",
    "    \"<p>下面表格是从小题和大题（如果多个小题同属于一个大题，大题的模型打分和老师打分为对应小题分数之和）两个层面统计的关于老师打分和模型打分之间分差的描述统计量</p>\",\n",
    "    \"<h3>小题层面</h3>\" + processed['html_ques_error'],\n",
    "    \"<h3>大题层面</h3>\" + processed['html_group_error'],\n",
    "    \n",
    "    \"<h2>判分准确性分析</h2>\",\n",
    "    \"\"\"<div class=\"description\">\n",
    "        <p>以下是从多个角度统计的模型判分准确性指标，其中精准匹配1/2/3是根据一下三种规则得到一个误差合理值或者误差合理范围，然后统计的分差在对应误差范围内的数据占比。规则如下：<br>\n",
    "           规则1：取题目最小得分点和该题目分值的10%两者之间的最小值；<br>\n",
    "           规则2：分差的百分之95%分位数；<br>\n",
    "           规则3：获取分差均值和标准差，计算均值正负一个标准差范围。</p>\n",
    "    </div>\"\"\",\n",
    "    \"<h3>小题层面</h3>\" + processed['html_ques_acc'],\n",
    "    \"<h3>大题层面</h3>\" + processed['html_group_acc'],\n",
    "    \n",
    "    \"<h2>按规则筛选数据后准确性结果</h2>\",\n",
    "    \"\"\"<div class=\"description\">\n",
    "        <p>以下结果是根据题目判分类型和观察模型判定错误类型及数据后总结的一些筛选规则，根据规则筛选出来的数据暂时认定为是模型判断无误的数据。<br>\n",
    "        表格展示的是小题和大题层面，各科目最终版本数据的各项指标准确率和基于最终版本和筛选条件后计算的准确率结果的对比。<br>\n",
    "        黑色代表两者准确率没有差异，红色代表处理后数据的准确率下降，绿色代表处理后数据的准确率上升。<br>\n",
    "           筛选条件1：剔除财管中题目类型为“表格”且模型判分为0的数据（可能是学生答案没填在表格里，没法判）；<br>\n",
    "           筛选条件2：财管中如果某个小题错误类型都是单位错误，暂时认定该题目模型打分和老师判分没分差；<br>\n",
    "           筛选条件3：战略中有错误类型包含“正确答案已经全部匹配”字样，暂时认定该题目模型打分和老师判分没分差</p>\n",
    "    </div>\"\"\",\n",
    "    \"<h3>小题层面</h3>\" + processed['compare_ques_acc'],\n",
    "    \"<h3>大题层面</h3>\" + processed['compare_group_acc'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16781ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### 绘图相关函数 ##########\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "matplotlib.rcParams['font.family'] = ['SimHei']           # 使用黑体显示中文\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False         # 正确显示负号\n",
    "\n",
    "def create_kde_plot(ax, data, title, STYLE):\n",
    "    \"\"\"通用KDE绘图函数\"\"\"\n",
    "    if not data.empty:\n",
    "        for role in ['teacher', 'model']:\n",
    "            color, label = STYLE['colors'][role]\n",
    "            sns.kdeplot(data[f'{role}_score'], ax=ax, \n",
    "                        label=label, color=color, \n",
    "                        fill=True, warn_singular=False)\n",
    "        ax.set_title(title, fontsize=STYLE['subtitle_fontsize'])\n",
    "        ax.set_xlabel(\"得分\", fontsize=STYLE['label_fontsize'])\n",
    "        ax.set_ylabel(\"密度\", fontsize=STYLE['label_fontsize'])\n",
    "        ax.legend(fontsize=STYLE['legend_fontsize'])\n",
    "        ax.tick_params(axis='both', labelsize=STYLE['tick_fontsize'])\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "def build_grid_plot(df, groupby_col, title, filename, STYLE, output_dir):\n",
    "    \"\"\"通用网格绘图函数\"\"\"\n",
    "    # 准备数据\n",
    "    subjects = sorted(df['subject_name'].unique())\n",
    "    group_dict = {\n",
    "        subj: sorted(df[df['subject_name'] == subj][groupby_col].unique())\n",
    "        for subj in subjects\n",
    "    }\n",
    "    max_cols = max(len(v) for v in group_dict.values())\n",
    "    \n",
    "    # 创建画布\n",
    "    fig = plt.figure(\n",
    "        figsize=(STYLE['col_width'] * max_cols, \n",
    "                STYLE['row_height'] * len(subjects)),\n",
    "        constrained_layout=True\n",
    "    )\n",
    "    outer = gridspec.GridSpec(\n",
    "        len(subjects), 1, \n",
    "        height_ratios=[1]*len(subjects), \n",
    "        hspace=STYLE['hspace']\n",
    "    )\n",
    "\n",
    "    # 绘制子图\n",
    "    for row_idx, subject in enumerate(subjects):\n",
    "        groups = group_dict[subject]\n",
    "        inner = gridspec.GridSpecFromSubplotSpec(\n",
    "            1, len(groups), \n",
    "            subplot_spec=outer[row_idx], \n",
    "            wspace=STYLE['wspace']\n",
    "        )\n",
    "        \n",
    "        for k, group_val in enumerate(groups):\n",
    "            ax = plt.Subplot(fig, inner[k])\n",
    "            sub_df = df[(df['subject_name'] == subject) & \n",
    "                        (df[groupby_col] == group_val)]\n",
    "            \n",
    "            # 动态生成标题\n",
    "            if groupby_col == 'paper_id':\n",
    "                plot_title = f\"{subject} | 试卷: {group_val} | n={len(sub_df)}\"\n",
    "            elif groupby_col == 'question_score':\n",
    "                plot_title = f\"{subject} | 满分 {group_val} 分\\nn={len(sub_df)}\"\n",
    "            else:\n",
    "                plot_title = f\"{subject} | 题型: {group_val}\\nn={len(sub_df)}\"\n",
    "            \n",
    "            create_kde_plot(ax, sub_df, plot_title, STYLE)\n",
    "            fig.add_subplot(ax)\n",
    "\n",
    "    plt.suptitle(title, fontsize=STYLE['title_fontsize'], y=0.98)\n",
    "    plt.savefig(output_dir / filename, bbox_inches='tight', dpi=STYLE['dpi'])\n",
    "    return output_dir / filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc2770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### 生成html文件 ##############\n",
    "\n",
    "# 设置输出目录\n",
    "output_dir = Path(\"report_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. 保存所有图像\n",
    "def save_all_figures(final_df1):\n",
    "    \"\"\"保存三种评分分布对比图\"\"\"\n",
    "    # 常量定义（统一调整所有图表参数）\n",
    "    STYLE = {\n",
    "        'row_height': 4,          # 每行高度\n",
    "        'col_width': 5,           # 每列宽度\n",
    "        'title_fontsize': 20,      # 主标题字号\n",
    "        'subtitle_fontsize': 12,   # 子标题字号\n",
    "        'label_fontsize': 10,      # 轴标签字号\n",
    "        'legend_fontsize': 9,      # 图例字号\n",
    "        'tick_fontsize': 9,        # 刻度字号\n",
    "        'hspace': 0.5,            # 行间距\n",
    "        'wspace': 0.4,            # 列间距\n",
    "        'dpi': 300,               # 输出分辨率\n",
    "        'colors': {               # 颜色配置\n",
    "            'teacher': ('blue', '教师评分'),\n",
    "            'model': ('red', '模型评分')\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 主流程\n",
    "    fig_paths = []\n",
    "    df = final_df1[final_df1['paper_id'].isin(paper_range)].copy()\n",
    "    \n",
    "    # 图表1：按试卷分布\n",
    "    fig_paths.append(\n",
    "        build_grid_plot(\n",
    "            df, 'paper_id',\n",
    "            \"教师评分 vs 模型评分 分布图\",\n",
    "            \"fig_score_dist_by_paper.png\",\n",
    "            STYLE,\n",
    "            output_dir\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 图表2：按题目分值分布\n",
    "    fig_paths.append(\n",
    "        build_grid_plot(\n",
    "            df, 'question_score',\n",
    "            \"各科目不同题目分值下 教师 vs 模型 评分分布图\",\n",
    "            \"fig_score_dist_by_question_score.png\",\n",
    "            STYLE,\n",
    "            output_dir\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 图表3：按判分类型分布\n",
    "    fig_paths.append(\n",
    "        build_grid_plot(\n",
    "            df, 'question_type',\n",
    "            \"各科目不同判分类型下 教师 vs 模型 评分分布图\",\n",
    "            \"fig_score_dist_by_question_type.png\",\n",
    "            STYLE,\n",
    "            output_dir\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #    # 图表3：按判分类型分布\n",
    "    # fig_paths.append(\n",
    "    #     build_grid_plot(\n",
    "    #         df, 'question_type',\n",
    "    #         \"各科目不同错误类型下 教师 vs 模型 评分分布图\",\n",
    "    #         \"fig_score_dist_by_question_type.png\",\n",
    "    #         STYLE,\n",
    "    #         output_dir\n",
    "    #     )\n",
    "    # )\n",
    "    \n",
    "    return fig_paths\n",
    "fig_paths = save_all_figures(final_df1)\n",
    "\n",
    "# 2. 表格 HTML（你之前生成的）\n",
    "table_htmls = table_html_list\n",
    "\n",
    "# 3. 合并 HTML 结构\n",
    "full_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"zh\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>主观题评分误差分析报告</title>\n",
    "    <style>\n",
    "        body { font-family: \"Microsoft YaHei\", sans-serif; padding: 40px; line-height: 1.6; }\n",
    "        h2 { margin-top: 40px; }\n",
    "        img { max-width: 100%; height: auto; margin: 20px 0; border: 1px solid #ddd; }\n",
    "        table { border-collapse: collapse; width: 100%; margin: 10px 0; }\n",
    "        th, td { border: 1px solid #ccc; padding: 5px; text-align: center; }\n",
    "        .group-end td { border-bottom: 2px solid #999; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>主观题评分误差分析报告</h1>\n",
    "\n",
    "    <p>本报告汇总了模型评分与教师评分在多个维度下的误差统计分析，包括不同试卷、题型、分值下的分布可视化和一致性对比。</p>\n",
    "\"\"\"\n",
    "\n",
    "for table_html in table_htmls:\n",
    "    full_html += f\"<div>{table_html}</div>\"\n",
    "\n",
    "# 添加图片部分\n",
    "full_html += \"<h2>不同条件下两者打分对比</h2>\"\n",
    "for fig_path in fig_paths:\n",
    "    full_html += f'<img src=\"{fig_path.name}\" alt=\"图表\">'\n",
    "\n",
    "full_html += \"</body></html>\"\n",
    "\n",
    "# 4. 写入 HTML 文件\n",
    "html_path = output_dir / \"report.html\"\n",
    "with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(full_html)\n",
    "\n",
    "print(f\"✅ 报告已生成：{html_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
